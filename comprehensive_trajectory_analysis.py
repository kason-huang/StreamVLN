#!/usr/bin/env python3
"""
StreamVLNè½¨è¿¹æ•°æ®ç»¼åˆåˆ†ææŠ¥å‘Š
åŒ…å«æ‰€æœ‰æ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡å’Œå¯¹æ¯”
"""

import json
import os
import pandas as pd
from datetime import datetime

def load_dataset_stats(dataset_name, file_path):
    """åŠ è½½å•ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading {dataset_name}: {e}")
        return None

    if not isinstance(data, list):
        data = [data]

    total_episodes = len(data)
    action_lengths = []
    instruction_counts = []

    for episode in data:
        if 'actions' in episode:
            action_lengths.append(len(episode['actions']))
        if 'instructions' in episode:
            instructions = episode['instructions']
            if isinstance(instructions, list):
                instruction_counts.append(len(instructions))
            else:
                instruction_counts.append(1)

    return {
        'dataset': dataset_name,
        'total_episodes': total_episodes,
        'avg_action_length': sum(action_lengths) / len(action_lengths) if action_lengths else 0,
        'min_action_length': min(action_lengths) if action_lengths else 0,
        'max_action_length': max(action_lengths) if action_lengths else 0,
        'total_instructions': sum(instruction_counts),
        'avg_instructions': sum(instruction_counts) / len(instruction_counts) if instruction_counts else 0
    }

def create_comprehensive_report():
    """åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š"""

    trajectory_data_path = "/root/workspace/lab/StreamVLN/data/trajectory_data"

    # æ‰€æœ‰æ•°æ®é›†è·¯å¾„
    datasets = {
        'R2R': os.path.join(trajectory_data_path, 'R2R', 'annotations.json'),
        'RxR': os.path.join(trajectory_data_path, 'RxR', 'annotations.json'),
        'EnvDrop': os.path.join(trajectory_data_path, 'EnvDrop', 'annotations.json'),
        'ScaleVLN': os.path.join(trajectory_data_path, 'ScaleVLN', 'annotations.json')
    }

    print("StreamVLN è½¨è¿¹æ•°æ®ç»¼åˆåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # åŠ è½½æ‰€æœ‰æ•°æ®é›†ç»Ÿè®¡
    all_stats = []
    for dataset_name, file_path in datasets.items():
        if os.path.exists(file_path):
            stats = load_dataset_stats(dataset_name, file_path)
            if stats:
                all_stats.append(stats)
                print(f"âœ… {dataset_name}: {stats['total_episodes']:,} æ¡è½¨è¿¹")
        else:
            print(f"âŒ {dataset_name}: æ–‡ä»¶ä¸å­˜åœ¨")

    print()

    # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
    df = pd.DataFrame(all_stats)

    # æ€»ä½“ç»Ÿè®¡
    total_episodes = df['total_episodes'].sum()
    total_instructions = df['total_instructions'].sum()

    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print("-" * 30)
    print(f"æ€»è½¨è¿¹æ•°: {total_episodes:,}")
    print(f"æ€»æŒ‡ä»¤æ•°: {total_instructions:,}")
    print(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}")
    print()

    # æ’é™¤ScaleVLNçš„ç»Ÿè®¡
    non_scalevln_df = df[df['dataset'] != 'ScaleVLN']
    non_scalevln_episodes = non_scalevln_df['total_episodes'].sum()
    non_scalevln_instructions = non_scalevln_df['total_instructions'].sum()

    print("ğŸ“Š æ’é™¤ScaleVLNçš„ç»Ÿè®¡")
    print("-" * 30)
    print(f"è½¨è¿¹æ•°: {non_scalevln_episodes:,}")
    print(f"æŒ‡ä»¤æ•°: {non_scalevln_instructions:,}")
    print(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {non_scalevln_instructions/non_scalevln_episodes:.2f}")
    print()

    # è¯¦ç»†æ•°æ®é›†å¯¹æ¯”
    print("ğŸ“‹ è¯¦ç»†æ•°æ®é›†å¯¹æ¯”")
    print("-" * 30)
    print(df[['dataset', 'total_episodes', 'avg_action_length', 'avg_instructions']].to_string(index=False))
    print()

    # æ•°æ®åˆ†å¸ƒåˆ†æ
    print("ğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ")
    print("-" * 30)

    for _, row in df.iterrows():
        dataset_name = row['dataset']
        percentage = (row['total_episodes'] / total_episodes) * 100
        print(f"{dataset_name:10s}: {row['total_episodes']:8,} ({percentage:5.2f}%) - å¹³å‡åŠ¨ä½œé•¿åº¦: {row['avg_action_length']:6.2f}")

    print()

    # å¯¹Stage 1è®­ç»ƒçš„å¯ç¤º
    print("ğŸ¯ Stage 1 è®­ç»ƒæ•°æ®åˆ†æ")
    print("-" * 30)
    print("åŸºäºåˆ†æç»“æœï¼ŒStage 1è®­ç»ƒå¯ä»¥è€ƒè™‘ä»¥ä¸‹ç­–ç•¥:")
    print()

    # ä¸»è¦æ•°æ®é›† (æ’é™¤ScaleVLN)
    main_datasets = ['R2R', 'RxR', 'EnvDrop']
    main_total = non_scalevln_df[non_scalevln_df['dataset'].isin(main_datasets)]['total_episodes'].sum()

    print(f"1. ä¸»è¦è®­ç»ƒæ•°æ®é›† (æ’é™¤ScaleVLN):")
    print(f"   - æ€»è½¨è¿¹æ•°: {main_total:,}")
    print(f"   - å¹³å‡åŠ¨ä½œé•¿åº¦: {non_scalevln_df[non_scalevln_df['dataset'].isin(main_datasets)]['avg_action_length'].mean():.2f}")
    print(f"   - è¿™äº›æ•°æ®è´¨é‡é«˜ï¼Œé€‚åˆä½œä¸ºé¢„è®­ç»ƒçš„ä¸»è¦æ•°æ®æº")
    print()

    print("2. ScaleVLNæ•°æ®çš„ç‰¹ç‚¹:")
    scalevln_stats = df[df['dataset'] == 'ScaleVLN'].iloc[0]
    print(f"   - è½¨è¿¹æ•°: {scalevln_stats['total_episodes']:,}")
    print(f"   - å¹³å‡åŠ¨ä½œé•¿åº¦è¾ƒçŸ­: {scalevln_stats['avg_action_length']:.2f}")
    print(f"   - å»ºè®®åœ¨Daggeré˜¶æ®µæˆ–åæœŸè®­ç»ƒä¸­å¼•å…¥ï¼Œä½œä¸ºå¤§è§„æ¨¡æ•°æ®å¢å¼º")
    print()

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    output_file = "/root/workspace/lab/StreamVLN/comprehensive_trajectory_report.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("StreamVLN è½¨è¿¹æ•°æ®ç»¼åˆåˆ†ææŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ€»è½¨è¿¹æ•°: {total_episodes:,}\n")
        f.write(f"æ€»æŒ‡ä»¤æ•°: {total_instructions:,}\n")
        f.write(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}\n\n")

        f.write("ğŸ“Š æ’é™¤ScaleVLNçš„ç»Ÿè®¡ (Stage 1ä¸»è¦æ•°æ®)\n")
        f.write("-" * 30 + "\n")
        f.write(f"è½¨è¿¹æ•°: {non_scalevln_episodes:,}\n")
        f.write(f"æŒ‡ä»¤æ•°: {non_scalevln_instructions:,}\n")
        f.write(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {non_scalevln_instructions/non_scalevln_episodes:.2f}\n\n")

        f.write("ğŸ“‹ è¯¦ç»†æ•°æ®é›†å¯¹æ¯”\n")
        f.write("-" * 30 + "\n")
        f.write(df[['dataset', 'total_episodes', 'avg_action_length', 'avg_instructions']].to_string(index=False))
        f.write("\n\n")

        f.write("ğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ\n")
        f.write("-" * 30 + "\n")
        for _, row in df.iterrows():
            dataset_name = row['dataset']
            percentage = (row['total_episodes'] / total_episodes) * 100
            f.write(f"{dataset_name:10s}: {row['total_episodes']:8,} ({percentage:5.2f}%) - å¹³å‡åŠ¨ä½œé•¿åº¦: {row['avg_action_length']:6.2f}\n")

        f.write("\nğŸ¯ Stage 1 è®­ç»ƒå»ºè®®\n")
        f.write("-" * 30 + "\n")
        f.write("1. ä½¿ç”¨R2R + RxR + EnvDropä½œä¸ºä¸»è¦è®­ç»ƒæ•°æ® (æ’é™¤ScaleVLN)\n")
        f.write("2. æ€»è®¡çº¦15.6ä¸‡æ¡é«˜è´¨é‡è½¨è¿¹æ•°æ®\n")
        f.write("3. ScaleVLNåœ¨Daggeré˜¶æ®µæˆ–åæœŸå¼•å…¥\n")
        f.write("4. æ³¨æ„æ•°æ®ä¸å¹³è¡¡ï¼Œå¯èƒ½éœ€è¦é‡‡æ ·ç­–ç•¥\n")

    print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    return {
        'total_all': total_episodes,
        'total_excluding_scalevln': non_scalevln_episodes,
        'dataset_stats': all_stats
    }

if __name__ == "__main__":
    results = create_comprehensive_report()