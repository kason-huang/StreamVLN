#!/usr/bin/env python3
"""
åˆ†æStreamVLN trajectory_dataç›®å½•ä¸­çš„è½¨è¿¹æ•°æ®ç»Ÿè®¡
æ’é™¤ScaleVLNï¼Œç»Ÿè®¡R2Rã€RxRã€EnvDropçš„è½¨è¿¹æ•°æ®é‡
"""

import json
import os
from collections import defaultdict
import pandas as pd

def load_annotations(file_path):
    """åŠ è½½annotations.jsonæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None

def analyze_dataset(dataset_name, file_path):
    """åˆ†æå•ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n=== åˆ†æ {dataset_name} æ•°æ®é›† ===")

    data = load_annotations(file_path)
    if data is None:
        return None

    if not isinstance(data, list):
        data = [data]

    # åŸºç¡€ç»Ÿè®¡
    total_episodes = len(data)

    # åœºæ™¯ç»Ÿè®¡
    scenes = set()
    scene_episode_counts = defaultdict(int)  # ç»Ÿè®¡æ¯ä¸ªåœºæ™¯çš„episodeæ•°é‡

    # åŠ¨ä½œåºåˆ—ç»Ÿè®¡
    action_lengths = []
    instruction_counts = []
    trajectory_analysis = defaultdict(int)

    for episode in data:
        # ç»Ÿè®¡åŠ¨ä½œåºåˆ—é•¿åº¦
        if 'actions' in episode:
            actions = episode['actions']
            action_lengths.append(len(actions))

        # ç»Ÿè®¡æŒ‡ä»¤æ•°é‡
        if 'instructions' in episode:
            instructions = episode['instructions']
            if isinstance(instructions, list):
                instruction_counts.append(len(instructions))
            else:
                instruction_counts.append(1)

        # ç»Ÿè®¡åœºæ™¯åç§°
        if 'video' in episode:
            video_path = episode['video']
            # videoæ ¼å¼: "images/{scene_name}_{data_source}_{id}"
            # æå–scene_name
            if video_path.startswith('images/'):
                video_name = video_path[7:]  # å»æ‰"images/"å‰ç¼€
                parts = video_name.split('_')
                if len(parts) >= 2:
                    scene_name = parts[0]
                    scenes.add(scene_name)
                    scene_episode_counts[scene_name] += 1

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        has_video = 'video' in episode
        has_actions = 'actions' in episode
        has_instructions = 'instructions' in episode

        trajectory_analysis['has_video'] += int(has_video)
        trajectory_analysis['has_actions'] += int(has_actions)
        trajectory_analysis['has_instructions'] += int(has_instructions)
        trajectory_analysis['complete'] += int(has_video and has_actions and has_instructions)

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    # æŒ‰episodeæ•°é‡æ’åºçš„åœºæ™¯åˆ—è¡¨
    sorted_scenes = sorted(scene_episode_counts.items(), key=lambda x: x[1], reverse=True)

    stats = {
        'dataset': dataset_name,
        'total_episodes': total_episodes,
        'unique_scenes': len(scenes),
        'avg_action_length': sum(action_lengths) / len(action_lengths) if action_lengths else 0,
        'min_action_length': min(action_lengths) if action_lengths else 0,
        'max_action_length': max(action_lengths) if action_lengths else 0,
        'avg_instructions': sum(instruction_counts) / len(instruction_counts) if instruction_counts else 0,
        'total_instructions': sum(instruction_counts),
        'complete_episodes': trajectory_analysis['complete'],
        'data_completeness_rate': trajectory_analysis['complete'] / total_episodes * 100 if total_episodes > 0 else 0,
        'scenes': sorted(scenes),  # ä¿å­˜åœºæ™¯åˆ—è¡¨ç”¨äºè¯¦ç»†æŠ¥å‘Š
        'scene_episode_counts': scene_episode_counts,  # ä¿å­˜æ¯ä¸ªåœºæ™¯çš„episodeæ•°é‡
        'top_scenes': sorted_scenes[:10]  # ä¿å­˜episodeæ•°é‡æœ€å¤šçš„å‰10ä¸ªåœºæ™¯
    }

    print(f"æ€»è½¨è¿¹æ•°: {stats['total_episodes']:,}")
    print(f"å®Œæ•´è½¨è¿¹æ•°: {stats['complete_episodes']:,}")
    print(f"æ•°æ®å®Œæ•´ç‡: {stats['data_completeness_rate']:.2f}%")
    print(f"ä¸åŒåœºæ™¯æ•°: {stats['unique_scenes']:,}")
    print(f"å¹³å‡åŠ¨ä½œåºåˆ—é•¿åº¦: {stats['avg_action_length']:.2f}")
    print(f"åŠ¨ä½œåºåˆ—é•¿åº¦èŒƒå›´: {stats['min_action_length']} - {stats['max_action_length']}")
    print(f"æ€»æŒ‡ä»¤æ•°: {stats['total_instructions']:,}")
    print(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {stats['avg_instructions']:.2f}")

    # æ˜¾ç¤ºepisodeæ•°é‡æœ€å¤šçš„å‰5ä¸ªåœºæ™¯
    print(f"\nTop 5 åœºæ™¯ (æŒ‰episodeæ•°é‡):")
    for scene_name, episode_count in stats['top_scenes'][:5]:
        avg_episodes = episode_count
        print(f"  - {scene_name}: {episode_count} episodes")

    return stats

def analyze_trajectory_data():
    """ä¸»å‡½æ•°ï¼šåˆ†ætrajectory_dataç›®å½•"""

    trajectory_data_path = "/root/workspace/lab/StreamVLN/data/trajectory_data"

    if not os.path.exists(trajectory_data_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç›®å½• {trajectory_data_path}")
        return

    # è¦åˆ†æçš„æ•°æ®é›†ï¼ˆæ’é™¤ScaleVLNï¼‰
    datasets = {
        'R2R': os.path.join(trajectory_data_path, 'R2R', 'annotations.json'),
        'RxR': os.path.join(trajectory_data_path, 'RxR', 'annotations.json'),
        'EnvDrop': os.path.join(trajectory_data_path, 'EnvDrop', 'annotations.json'),
        # æ³¨é‡Šæ‰ScaleVLNï¼Œæ ¹æ®è¦æ±‚æ’é™¤
        # 'ScaleVLN': os.path.join(trajectory_data_path, 'ScaleVLN', 'annotations.json')
    }

    print("å¼€å§‹åˆ†ætrajectory_dataç›®å½•ä¸­çš„è½¨è¿¹æ•°æ®...")
    print("(æ’é™¤ScaleVLNæ•°æ®é›†)")

    all_stats = []
    total_episodes = 0
    total_instructions = 0
    total_complete_episodes = 0
    all_scenes = set()

    for dataset_name, file_path in datasets.items():
        if os.path.exists(file_path):
            stats = analyze_dataset(dataset_name, file_path)
            if stats:
                all_stats.append(stats)
                total_episodes += stats['total_episodes']
                total_instructions += stats['total_instructions']
                total_complete_episodes += stats['complete_episodes']
                # æ”¶é›†æ‰€æœ‰åœºæ™¯åç§°
                if 'scenes' in stats:
                    all_scenes.update(stats['scenes'])
        else:
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("æ€»ä½“ç»Ÿè®¡æŠ¥å‘Š (æ’é™¤ScaleVLN)")
    print(f"{'='*60}")

    print(f"\nğŸ“Š æ•°æ®é›†æ±‡æ€»:")
    print(f"  å‚ä¸ç»Ÿè®¡çš„æ•°æ®é›†æ•°é‡: {len(all_stats)}")
    for stats in all_stats:
        print(f"  - {stats['dataset']}: {stats['total_episodes']:,} æ¡è½¨è¿¹, {stats['unique_scenes']:,} ä¸ªåœºæ™¯")

    print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
    print(f"  æ€»è½¨è¿¹æ•°: {total_episodes:,}")
    print(f"  å®Œæ•´è½¨è¿¹æ•°: {total_complete_episodes:,}")
    print(f"  æ€»ä½“å®Œæ•´ç‡: {total_complete_episodes/total_episodes*100:.2f}%")
    print(f"  æ€»æŒ‡ä»¤æ•°: {total_instructions:,}")
    print(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}")
    print(f"  æ€»åœºæ™¯æ•°: {len(all_scenes):,}")

    # ç”Ÿæˆæ•°æ®æ¡†ä¾¿äºæŸ¥çœ‹
    if all_stats:
        df = pd.DataFrame(all_stats)

        print(f"\nğŸ“‹ è¯¦ç»†ç»Ÿè®¡è¡¨:")
        print(df[['dataset', 'total_episodes', 'unique_scenes', 'complete_episodes', 'data_completeness_rate',
                  'avg_action_length', 'avg_instructions']].to_string(index=False))

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = "/root/workspace/lab/StreamVLN/trajectory_analysis_report.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("StreamVLN Trajectory Data Analysis Report\n")
            f.write("(æ’é™¤ScaleVLNæ•°æ®é›†)\n")
            f.write("="*50 + "\n\n")

            for stats in all_stats:
                f.write(f"{stats['dataset']} æ•°æ®é›†:\n")
                f.write(f"  æ€»è½¨è¿¹æ•°: {stats['total_episodes']:,}\n")
                f.write(f"  ä¸åŒåœºæ™¯æ•°: {stats['unique_scenes']:,}\n")
                f.write(f"  å®Œæ•´è½¨è¿¹æ•°: {stats['complete_episodes']:,}\n")
                f.write(f"  æ•°æ®å®Œæ•´ç‡: {stats['data_completeness_rate']:.2f}%\n")
                f.write(f"  å¹³å‡åŠ¨ä½œåºåˆ—é•¿åº¦: {stats['avg_action_length']:.2f}\n")
                f.write(f"  æ€»æŒ‡ä»¤æ•°: {stats['total_instructions']:,}\n")
                f.write(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {stats['avg_instructions']:.2f}\n")

                # å†™å…¥åœºæ™¯åˆ—è¡¨
                if 'scenes' in stats and stats['scenes']:
                    f.write(f"  åœºæ™¯åˆ—è¡¨: {', '.join(stats['scenes'][:10])}")
                    if len(stats['scenes']) > 10:
                        f.write(f" ... (å…±{len(stats['scenes'])}ä¸ªåœºæ™¯)")
                    f.write("\n")

                # å†™å…¥Top 10 åœºæ™¯ç»Ÿè®¡
                if 'top_scenes' in stats and stats['top_scenes']:
                    f.write(f"  Top 10 åœºæ™¯ (æŒ‰episodeæ•°é‡):\n")
                    for i, (scene_name, episode_count) in enumerate(stats['top_scenes'], 1):
                        f.write(f"    {i}. {scene_name}: {episode_count} episodes\n")
                f.write("\n")

            f.write("æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  æ€»è½¨è¿¹æ•°: {total_episodes:,}\n")
            f.write(f"  æ€»åœºæ™¯æ•°: {len(all_scenes):,}\n")
            f.write(f"  å®Œæ•´è½¨è¿¹æ•°: {total_complete_episodes:,}\n")
            f.write(f"  æ€»ä½“å®Œæ•´ç‡: {total_complete_episodes/total_episodes*100:.2f}%\n")
            f.write(f"  æ€»æŒ‡ä»¤æ•°: {total_instructions:,}\n")
            f.write(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}\n")

        # ç”Ÿæˆåœºæ™¯episodeç»Ÿè®¡CSVæ–‡ä»¶
        csv_file = "/root/workspace/lab/StreamVLN/scene_episode_statistics.csv"
        with open(csv_file, 'w', encoding='utf-8') as f_csv:
            f_csv.write("Dataset,Scene,EpisodeCount,Percentage\n")
            for stats in all_stats:
                dataset_name = stats['dataset']
                total_episodes_dataset = stats['total_episodes']
                if 'scene_episode_counts' in stats:
                    # æŒ‰episodeæ•°é‡é™åºæ’åˆ—
                    sorted_scene_counts = sorted(stats['scene_episode_counts'].items(), key=lambda x: x[1], reverse=True)
                    for scene_name, episode_count in sorted_scene_counts:
                        percentage = (episode_count / total_episodes_dataset) * 100
                        f_csv.write(f"{dataset_name},{scene_name},{episode_count},{percentage:.2f}%\n")

        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ’¾ åœºæ™¯episodeç»Ÿè®¡å·²ä¿å­˜åˆ°: {csv_file}")

    return {
        'total_episodes': total_episodes,
        'total_instructions': total_instructions,
        'total_complete_episodes': total_complete_episodes,
        'total_scenes': len(all_scenes),
        'all_scenes': sorted(all_scenes),
        'dataset_stats': all_stats
    }

if __name__ == "__main__":
    results = analyze_trajectory_data()