#!/usr/bin/env python3
"""
åˆ†æStreamVLN trajectory_dataç›®å½•ä¸­çš„è½¨è¿¹æ•°æ®ç»Ÿè®¡
æ’é™¤ScaleVLNï¼Œç»Ÿè®¡R2Rã€RxRã€EnvDropçš„è½¨è¿¹æ•°æ®é‡
"""

import json
import os
from collections import defaultdict
import pandas as pd

def load_annotations(file_path):
    """åŠ è½½annotations.jsonæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None

def analyze_dataset(dataset_name, file_path):
    """åˆ†æå•ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n=== åˆ†æ {dataset_name} æ•°æ®é›† ===")

    data = load_annotations(file_path)
    if data is None:
        return None

    if not isinstance(data, list):
        data = [data]

    # åŸºç¡€ç»Ÿè®¡
    total_episodes = len(data)

    # åŠ¨ä½œåºåˆ—ç»Ÿè®¡
    action_lengths = []
    instruction_counts = []
    trajectory_analysis = defaultdict(int)

    for i, episode in enumerate(data):
        # ç»Ÿè®¡åŠ¨ä½œåºåˆ—é•¿åº¦
        if 'actions' in episode:
            actions = episode['actions']
            action_lengths.append(len(actions))

        # ç»Ÿè®¡æŒ‡ä»¤æ•°é‡
        if 'instructions' in episode:
            instructions = episode['instructions']
            if isinstance(instructions, list):
                instruction_counts.append(len(instructions))
            else:
                instruction_counts.append(1)

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        has_video = 'video' in episode
        has_actions = 'actions' in episode
        has_instructions = 'instructions' in episode

        trajectory_analysis['has_video'] += int(has_video)
        trajectory_analysis['has_actions'] += int(has_actions)
        trajectory_analysis['has_instructions'] += int(has_instructions)
        trajectory_analysis['complete'] += int(has_video and has_actions and has_instructions)

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    stats = {
        'dataset': dataset_name,
        'total_episodes': total_episodes,
        'avg_action_length': sum(action_lengths) / len(action_lengths) if action_lengths else 0,
        'min_action_length': min(action_lengths) if action_lengths else 0,
        'max_action_length': max(action_lengths) if action_lengths else 0,
        'avg_instructions': sum(instruction_counts) / len(instruction_counts) if instruction_counts else 0,
        'total_instructions': sum(instruction_counts),
        'complete_episodes': trajectory_analysis['complete'],
        'data_completeness_rate': trajectory_analysis['complete'] / total_episodes * 100 if total_episodes > 0 else 0
    }

    print(f"æ€»è½¨è¿¹æ•°: {stats['total_episodes']:,}")
    print(f"å®Œæ•´è½¨è¿¹æ•°: {stats['complete_episodes']:,}")
    print(f"æ•°æ®å®Œæ•´ç‡: {stats['data_completeness_rate']:.2f}%")
    print(f"å¹³å‡åŠ¨ä½œåºåˆ—é•¿åº¦: {stats['avg_action_length']:.2f}")
    print(f"åŠ¨ä½œåºåˆ—é•¿åº¦èŒƒå›´: {stats['min_action_length']} - {stats['max_action_length']}")
    print(f"æ€»æŒ‡ä»¤æ•°: {stats['total_instructions']:,}")
    print(f"å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {stats['avg_instructions']:.2f}")

    return stats

def analyze_trajectory_data():
    """ä¸»å‡½æ•°ï¼šåˆ†ætrajectory_dataç›®å½•"""

    trajectory_data_path = "/root/workspace/lab/StreamVLN/data/trajectory_data"

    if not os.path.exists(trajectory_data_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç›®å½• {trajectory_data_path}")
        return

    # è¦åˆ†æçš„æ•°æ®é›†ï¼ˆæ’é™¤ScaleVLNï¼‰
    datasets = {
        'R2R': os.path.join(trajectory_data_path, 'R2R', 'annotations.json'),
        'RxR': os.path.join(trajectory_data_path, 'RxR', 'annotations.json'),
        'EnvDrop': os.path.join(trajectory_data_path, 'EnvDrop', 'annotations.json'),
        # æ³¨é‡Šæ‰ScaleVLNï¼Œæ ¹æ®è¦æ±‚æ’é™¤
        # 'ScaleVLN': os.path.join(trajectory_data_path, 'ScaleVLN', 'annotations.json')
    }

    print("å¼€å§‹åˆ†ætrajectory_dataç›®å½•ä¸­çš„è½¨è¿¹æ•°æ®...")
    print("(æ’é™¤ScaleVLNæ•°æ®é›†)")

    all_stats = []
    total_episodes = 0
    total_instructions = 0
    total_complete_episodes = 0

    for dataset_name, file_path in datasets.items():
        if os.path.exists(file_path):
            stats = analyze_dataset(dataset_name, file_path)
            if stats:
                all_stats.append(stats)
                total_episodes += stats['total_episodes']
                total_instructions += stats['total_instructions']
                total_complete_episodes += stats['complete_episodes']
        else:
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("æ€»ä½“ç»Ÿè®¡æŠ¥å‘Š (æ’é™¤ScaleVLN)")
    print(f"{'='*60}")

    print(f"\nğŸ“Š æ•°æ®é›†æ±‡æ€»:")
    print(f"  å‚ä¸ç»Ÿè®¡çš„æ•°æ®é›†æ•°é‡: {len(all_stats)}")
    for stats in all_stats:
        print(f"  - {stats['dataset']}: {stats['total_episodes']:,} æ¡è½¨è¿¹")

    print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
    print(f"  æ€»è½¨è¿¹æ•°: {total_episodes:,}")
    print(f"  å®Œæ•´è½¨è¿¹æ•°: {total_complete_episodes:,}")
    print(f"  æ€»ä½“å®Œæ•´ç‡: {total_complete_episodes/total_episodes*100:.2f}%")
    print(f"  æ€»æŒ‡ä»¤æ•°: {total_instructions:,}")
    print(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}")

    # ç”Ÿæˆæ•°æ®æ¡†ä¾¿äºæŸ¥çœ‹
    if all_stats:
        df = pd.DataFrame(all_stats)

        print(f"\nğŸ“‹ è¯¦ç»†ç»Ÿè®¡è¡¨:")
        print(df[['dataset', 'total_episodes', 'complete_episodes', 'data_completeness_rate',
                  'avg_action_length', 'avg_instructions']].to_string(index=False))

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = "/root/workspace/lab/StreamVLN/trajectory_analysis_report.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("StreamVLN Trajectory Data Analysis Report\n")
            f.write("(æ’é™¤ScaleVLNæ•°æ®é›†)\n")
            f.write("="*50 + "\n\n")

            for stats in all_stats:
                f.write(f"{stats['dataset']} æ•°æ®é›†:\n")
                f.write(f"  æ€»è½¨è¿¹æ•°: {stats['total_episodes']:,}\n")
                f.write(f"  å®Œæ•´è½¨è¿¹æ•°: {stats['complete_episodes']:,}\n")
                f.write(f"  æ•°æ®å®Œæ•´ç‡: {stats['data_completeness_rate']:.2f}%\n")
                f.write(f"  å¹³å‡åŠ¨ä½œåºåˆ—é•¿åº¦: {stats['avg_action_length']:.2f}\n")
                f.write(f"  æ€»æŒ‡ä»¤æ•°: {stats['total_instructions']:,}\n")
                f.write(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {stats['avg_instructions']:.2f}\n\n")

            f.write("æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  æ€»è½¨è¿¹æ•°: {total_episodes:,}\n")
            f.write(f"  å®Œæ•´è½¨è¿¹æ•°: {total_complete_episodes:,}\n")
            f.write(f"  æ€»ä½“å®Œæ•´ç‡: {total_complete_episodes/total_episodes*100:.2f}%\n")
            f.write(f"  æ€»æŒ‡ä»¤æ•°: {total_instructions:,}\n")
            f.write(f"  å¹³å‡æ¯æ¡è½¨è¿¹æŒ‡ä»¤æ•°: {total_instructions/total_episodes:.2f}\n")

        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    return {
        'total_episodes': total_episodes,
        'total_instructions': total_instructions,
        'total_complete_episodes': total_complete_episodes,
        'dataset_stats': all_stats
    }

if __name__ == "__main__":
    results = analyze_trajectory_data()